<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Ruby | IO0001]]></title>
  <link href="http://cthulhu.github.com/cthulhu/blog/categories/ruby/atom.xml" rel="self"/>
  <link href="http://cthulhu.github.com/cthulhu/"/>
  <updated>2013-04-24T12:58:39+03:00</updated>
  <id>http://cthulhu.github.com/cthulhu/</id>
  <author>
    <name><![CDATA[Stanislav O. Pogrebnyak]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Deployment of Ruby On Rails]]></title>
    <link href="http://cthulhu.github.com/cthulhu/blog/2013/04/19/deployment-of-ruby-on-rails/"/>
    <updated>2013-04-19T01:17:00+03:00</updated>
    <id>http://cthulhu.github.com/cthulhu/blog/2013/04/19/deployment-of-ruby-on-rails</id>
    <content type="html"><![CDATA[<p>From time to time I need to refresh a guide of Rails application production environment deployment.</p>

<p>My favourite OS is Linux Ubuntu. I prefer to use LTS versions, and today's LTS is 12.04 Precise Pangolin. You can download server distribution from <a href="http://ubuntu.com">Ubuntu official site</a>.</p>

<p>I will not cover OS installation process, since that's quite trivial and there are a lot of tutorials in the Internet about that. Besides, I can imagine that OS already installed on your VPS/PS/ whatever you have.</p>

<p><em>Note:</em> You might want to use chef or puppets for deployment automatization. And you are absolutelly right - recipes are the best way to DRY that as well. I will definitely go for it at some point.</p>

<p>So you have installed OS and logged in to the server via ssh.</p>

<p><em>Note:</em> Don't forget to disable passworded authorization and root login for your ssh server. Also it's nice to change ssh port from default one to something else, so the hackers will not be able to use simples atacks.</p>

<pre><code>sudo sed -i "/^Port / c Port 22222" /etc/ssh/sshd_config
sudo sed -i "/^PermitRootLogin / c PermitRootLogin no" /etc/ssh/sshd_config
</code></pre>

<p>Here is a set of commands you need to execute to install/configure needed software.</p>

<p>Let's make our system up to date:</p>

<pre><code>sudo apt-get update &amp;&amp; sudo apt-get upgrade
</code></pre>

<p>Now let's install software we will use (Git, compilers, archivers etc):</p>

<pre><code>sudo apt-get install git-core build-essential zlib1g-dev libssl-dev vim mc htop libmysqlclient-dev curl git-core ruby libpcre3-dev libxml2-dev libxml2 libxslt1-dev imagemagick libmagickcore-dev libmagickwand-dev autoconf byacc mysql-client libmysql-ruby libmysqlclient15-dev libmysqlclient-dev libmagickwand-dev imagemagick libmagickcore-dev zip screen curl nodejs install -y ghostscript imagemagick ffmpeg -y 
</code></pre>

<p>Now we need a web server. Personally I perfer nginx. It is fast, easy to use and configure. Someone prefers to installing from packages but I always do that from sources, since I can get the latest one and also I can configure modules I need.</p>

<p>So let's get it (at the moment of that blog the latest stable release is 1.2.8):</p>

<pre><code>sudo useradd -s /usr/sbin/nologin nginx
cd ~
wget http://nginx.org/download/nginx-1.2.8.tar.gz
tar -xvf nginx-1.2.8.tar.gz
cd nginx-1.2.8
./configure --prefix='/opt/nginx' --with-http_ssl_module --with-http_gzip_static_module --with-cc-opt='-Wno-error' --with-http_stub_status_module
make
sudo make install
cd ..
rm nginx-1.2.8 -rf
rm nginx-1.2.8.tar.gz
</code></pre>

<p>Few words about build configuration:</p>

<pre><code>--with-http_ssl_module - Application hosted there will have ssl encryption

--with-http_stub_status_module - I will use munin nginx-* plugins for requests monitoring etc.
</code></pre>

<p>You can read more about nginx configuration on <a href="http://nginx.org/">official site</a></p>

<p>Now we need a startup script (I have a gist for that):</p>

<p><div><script src='https://gist.github.com/5420306.js'></script>
<noscript><pre><code></code></pre></noscript></div>
</p>

<pre><code>cd ~
git clone https://gist.github.com/5420306.git
sudo cp 5420306/nginx /etc/init.d/nginx
sudo chmod 0755 /etc/init.d/nginx
rm 5420306 -rf
</code></pre>

<p>Now let's get ruby. I would recomend to use rvm even in production environment, since it will bring some flexibility to your setup. Also I recomend to use per user rvm installation rather than system wide.</p>

<pre><code> \curl -#L https://get.rvm.io | bash -s stable
</code></pre>

<p>Let's apply changes:</p>

<pre><code>source ~/.bashrc
</code></pre>

<p>And install latest (production) ruby version:</p>

<pre><code>rvm install 1.9.3-p392
</code></pre>

<p>I prefer to install release number rather than head. Since it's quite clear how to upgrade in case of new versions.</p>

<p>Now gemset (I prefer to use before setup hook for capistrano) but this also will work:</p>

<pre><code>rvm use 1.9.3@appname --create
</code></pre>

<p>Now let's connect nginx to our rails application. We can do that using several approaches (web servers). By popularity ATM:</p>

<ul>
<li><a href="http://code.macournoyer.com/thin/">Thin</a></li>
<li><a href="https://www.phusionpassenger.com/">Passenger</a></li>
<li><a href="http://unicorn.bogomips.org/">Unicorn</a></li>
<li><a href="http://pow.cx/">Pow</a></li>
<li><a href="http://puma.io/">Puma</a></li>
<li><a href="http://rainbows.rubyforge.org/">Rainbows</a> - Unicorn fork</li>
<li>Other options</li>
</ul>


<p>I perfer unicorn, but you might want to consider another option. Actualy I'm quite happy to have so much choices. Not every platform have such a wide range of different solutions.</p>

<p>Unicorn is able to listen UNIX sockets, that's very handy and we can start multiple workers for our application and connect to nginx's upstream.</p>

<p>Our application will be located at /home/#{username}/application/#{application_name}, so socket location will be /home/#{username}/application/#{application_name}/shared/sockets/uncorn.sock.</p>

<p>Don't forget to add nginx user to your #{username} group. For example, for user 'deployer' it will be:</p>

<pre><code>sudo usermod -a -G deployer nginx
</code></pre>

<p>Last stage is database. I use mysql in most cases. Here how you can get it:</p>

<pre><code>sudo apt-get install mysql-server mysql-client mysql-common
</code></pre>

<p>I will not cover mysql configuration here, since It is really depends on your cases.
Let's created database and user:</p>

<pre><code>mysql -u root -p -e "CREATE DATABASE \`application_production\` CHARACTER SET utf8 COLLATE utf8_general_ci;"
mysql -u root -p -e "CREATE USER 'application'@'localhost' IDENTIFIED BY 'some_pass';"
mysql -u root -p -e "GRANT ALL PRIVILEGES ON application_production.* TO 'application'@'localhost' WITH GRANT OPTION;""
</code></pre>

<p>That's it. Since now I have step by step tutorial, I will try to compose a chef recipes for that.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elastic MapReduce using streaming and ruby]]></title>
    <link href="http://cthulhu.github.com/cthulhu/blog/2013/03/04/elastic-mapreduce-using-streaming-and-ruby/"/>
    <updated>2013-03-04T01:57:00+02:00</updated>
    <id>http://cthulhu.github.com/cthulhu/blog/2013/03/04/elastic-mapreduce-using-streaming-and-ruby</id>
    <content type="html"><![CDATA[<p><a href="http://aws.amazon.com/elasticmapreduce/">Amazon Elastic MapReduce</a> - Is a service provided by amazon web services intended to provide facilities to build fast and easy to maintain big data calculations using MapReduce paradigm. Basically, Amazon EMR is a hosted Hadoop, at first glance. But in combination with aws API and other services, like S3, EMR becomes quite feature full solution. Hadoop requires some extra skills to deploy it locally. At least you need to be familar with JVM infrostructure. EMR allows you to abstract from some technical aspects like deploying cluster and managing nodes.</p>

<p>Also, which is nice about EMR that you can write MR functions in any language, not necessary in Java.</p>

<p>Before we will start to work on the topic itself, here are some requirements you need to meet to be able to use samples:</p>

<ul>
<li>  Ruby programming language skills - not much, since ruby is quite expressive language</li>
<li>  Amazon Web Services account - if you don't have one please create, subscribe for EMR (Elastic MapReduce) and S3</li>
</ul>


<p>The main goal is to be able to process huge amount of data (we will take log files from nginx server from one of my applications) and generate statistic.
We will investigate amount of hits per browser families per day. I would like to note that it is just an example of how you can use amazon EMR for such kind of cases.</p>

<p>About MapReduce concept you can read on different sources, starting from <a href="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/uk//archive/mapreduce-osdi04.pdf">Goolge's whitepaper</a>, <a href="http://en.wikipedia.org/wiki/MapReduce">wiki</a>, <a href="http://hadoop.apache.org/docs/r1.0.4/mapred_tutorial.html">Hadoop sources</a>.</p>

<p>I will not cover the theoretical principles of MR, since they covered quite well in sources above. Besides, I think It is easier to get that on realworld example. So please, read the references, at least article in wikipedia.</p>

<p>First our source file - I will use nginx access.log from demo server and the use case will be is to collect amount of hits per browser family. Basilcaly, I dont care about period, let's say we have 1 log file per day and we will use as input file that one only.</p>

<p>Every access.log line formated with "combined" format:</p>

<pre><code>'$remote_addr - $remote_user [$time_local]  '
    '"$request" $status $body_bytes_sent '
    '"$http_referer" "$http_user_agent"';
</code></pre>

<p>And in real life looks like this:</p>

<pre><code>127.0.0.1 - - [23/Apr/2013:00:10:02 +0100] "GET / HTTP/1.0" 403 168 "-" "Wget/1.12 (linux-gnu)"
193.169.146.189 - - [22/Apr/2013:09:07:12 +0100] "GET /login HTTP/1.1" 200 1651 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_3) AppleWebKit/536.28.10 (KHTML, like Gecko) Version/6.0.3 Safari/536.28.10"
83.161.144.255 - - [22/Apr/2013:09:08:22 +0100] "GET /login HTTP/1.1" 200 1648 "-" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31"
</code></pre>

<p>Your log files should be placed on amazon S3 so EMR will be able to get access to them.</p>

<p>For my test cases I used 1 bucket with next structure inside:</p>

<pre><code>bootstrap.sh  
browsers.rb
input  
output
</code></pre>

<p>bootstrap.sh - bash file for software installation on every EMR node. Basically, you can setup gems and libs, compile needed externall resources etc with it.</p>

<p>My file is very simple:</p>

<pre><code>#!/bin/bash

sudo apt-get -y install rubygems
</code></pre>

<p>Since rubygems depends on ruby, we will get it installed as well. I can imagine that in reallife cases bootstrap scripts will be much more complex.</p>

<p>browsers.rb - that is basically our map function. In my example it looks like:</p>

<p><div><script src='https://gist.github.com/5446482.js'></script>
<noscript><pre><code></code></pre></noscript></div>
</p>

<p>That example will never win first price, but any way it works and shows main principles.</p>

<p>The main idea behind is that data actually goes to mapper STDIN line by line, after that we parse the lines and generate the emission to the STDOUT. It is quit simple and can be done actually in any language even shell.</p>

<p>To get started go to aws console, your favourite region and select s3. After that create bucket, in my case it was emr.workshop. Upload workshop directory with content above to that bucket. As a result you should see some thing like this:</p>

<p><img src="/images/s3-emr.jpg" title="[width] [height]" ></p>

<p>Input folder should contains access.log file with data.</p>

<p><em>Note:</em> I prefer to create a separate user and group when do some actions in aws. That guaranties some rights and visibility isolation, especially when you have quite a lot of critical and important applications running under same account. You can easily do that with IAM(Identity and Access Management) and policies generators. I will not cover that topic here, since it's quite nice described in aws documentation.</p>

<p>Once we loaded input and created policies(you can skip that if you want), we can build our MR-job.</p>

<p>Go to aws elastic map reduce and click "Create New Job Flow". Fill the name of the job, in my case is "Browsers Analysis", select hadoop version "Amazon Distribution", Select "Run your own application" as a job flow.</p>

<p>Result should be like that:</p>

<p><img src="/images/create-job-step1.jpg" title="[width] [height]" ></p>

<p>On the next step you should fill in MR-job parameters. For me it was:</p>

<ul>
<li>Input location: s3://emr-workshop2013/input/access.log</li>
<li>Output location: s3://emr-workshop2013/output/</li>
<li>Mapper: s3://emr-workshop2013/browsers.rb</li>
<li>Reducer: aggregate</li>
</ul>


<p><img src="/images/create-job-step2.jpg" title="[width] [height]" ></p>

<p>Next you need to pick up hardware for the job. That is really depends on your case and expectations. If you have 100 Gb log file, and you want to process it faster than you probably need to think about more nodes. In our example everything is quite simple and log file is not very big so we will pick up few small instances(basically default options). You should also consider <a href="http://aws.amazon.com/elasticmapreduce/pricing/">costs</a> when you pick up size of computation cluster.</p>

<p>On Advanced options step you need to configure additional options for the job. I would say use defaults and press continue, since in test example all those settings do not matter much.</p>

<p>On next step you will have to configure bootstrap options for your nodes. All our nodes will be bootstrapped same using our script loaded to s3 bucket above. So Select "Configure your Bootstrap Actions", Action type: "Custom Action", Name: Node bootstrap. Amazon S3 Location: s3://emr-workshop2013/bootstrap.sh.</p>

<p><img src="/images/create-job-step5.jpg" title="[width] [height]" ></p>

<p>On last step of the wisard you will be able to review job's settings.</p>

<p>After you will press "Create Job Flow" Amazon will start instances and the computation will start. It will take few minutes to bootstrap the nodes, and ofcourse some time to process the data from input log file. As a result in output folder you will have a bunch of text files with aggregated counts per browser family.</p>

<p>How to debug MR-job?</p>

<p>Since job(mapper) gets data from STDIN you can easily do that with just piping in linux command line:</p>

<pre><code>cat input/access.log | ./browsers.rb
</code></pre>

<p>will give you output from the mapper, and with:</p>

<pre><code>cat input/access.log | ./browsers.rb | sort | uniq -c
</code></pre>

<p>You will have results, close to aggregate reducer output. In my case it was:</p>

<pre><code>      1 LongValueSum:Chrome ["10"]  1
     14 LongValueSum:Chrome ["11"]  1
      2 LongValueSum:Chrome ["12"]  1
      1 LongValueSum:Chrome ["14"]  1
      1 LongValueSum:Chrome ["15"]  1
      2 LongValueSum:Chrome ["16"]  1
      3 LongValueSum:Chrome ["17"]  1
     90 LongValueSum:Chrome ["18"]  1
      1 LongValueSum:Chrome ["20"]  1
      7 LongValueSum:Chrome ["21"]  1
     25 LongValueSum:Chrome ["22"]  1
   1826 LongValueSum:Chrome ["23"]  1
     10 LongValueSum:Chrome ["24"]  1
      3 LongValueSum:Chrome ["25"]  1
      1 LongValueSum:Chrome ["5"]   1
      1 LongValueSum:Chrome ["7"]   1
      1 LongValueSum:Chromium ["20"]    1
      1 LongValueSum:Chromium ["22"]    1
     15 LongValueSum:Firefox ["10"] 1
      8 LongValueSum:Firefox ["11"] 1
     26 LongValueSum:Firefox ["12"] 1
     13 LongValueSum:Firefox ["13"] 1
     30 LongValueSum:Firefox ["14"] 1
     23 LongValueSum:Firefox ["15"] 1
     56 LongValueSum:Firefox ["16"] 1
    940 LongValueSum:Firefox ["17"] 1
     31 LongValueSum:Firefox ["18"] 1
      2 LongValueSum:Firefox ["19"] 1
      1 LongValueSum:Firefox ["20"] 1
     22 LongValueSum:Firefox ["3"]  1
      1 LongValueSum:Firefox ["4"]  1
      1 LongValueSum:Firefox ["5"]  1
      4 LongValueSum:Firefox ["6"]  1
     27 LongValueSum:Firefox ["8"]  1
      9 LongValueSum:Firefox ["9"]  1
   3042 LongValueSum:Internet Explorer  1
     96 LongValueSum:Opera ["9"]    1
     65 LongValueSum:Safari     1
      1 LongValueSum:Safari ["3"]   1
    953 LongValueSum:Safari ["4"]   1
    683 LongValueSum:Safari ["5"]   1
   1407 LongValueSum:Safari ["6"]   1
     12 LongValueSum:Safari ["7"]   1
</code></pre>

<p>Also during MR-job creation you will be able to set bucket for MR process logging and ability to access nodes to check software etc(Step Advanced Settings).</p>

<p>This was very basic overview of EMR abilities, I hope in the nearest future I will be able to present some more complex usecases and applications of that technologie.</p>
]]></content>
  </entry>
  
</feed>
