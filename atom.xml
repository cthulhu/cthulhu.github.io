<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[IO0001]]></title>
  <link href="http://cthulhu.github.com/cthulhu/atom.xml" rel="self"/>
  <link href="http://cthulhu.github.com/cthulhu/"/>
  <updated>2013-11-13T00:06:21+01:00</updated>
  <id>http://cthulhu.github.com/cthulhu/</id>
  <author>
    <name><![CDATA[Stanislav O. Pogrebnyak]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[C* On Ubuntu Single-Node Cluster]]></title>
    <link href="http://cthulhu.github.com/cthulhu/blog/2013/11/10/c-star-on-ubuntu-single-node-cluster/"/>
    <updated>2013-11-10T23:07:00+01:00</updated>
    <id>http://cthulhu.github.com/cthulhu/blog/2013/11/10/c-star-on-ubuntu-single-node-cluster</id>
    <content type="html"><![CDATA[<p>Due to <a href="http://cassandra.apache.org/download/">http://cassandra.apache.org/download/</a> &ndash; The latest stable release of Apache Cassandra is 2.0.2 (released on 2013-10-28).</p>

<p>I&#8217;v googled quite a while to get the most true way of C* installation under the Ubuntu. No luck actually, for a some reason C* doesn&rsquo;t have any well supported PPA or so. And looks like the only way to get it is to download from site and go for a manual.</p>

<p>Update: DataStax hosts some Debian packages for C*, but 1 &ndash; they are looks old, 2 &ndash; they are for Debian not for Ubuntu. So lets go for a manual.</p>

<p>Update 2: My colleague showed me that <a href="http://www.datastax.com/documentation/gettingstarted/getting_started/gettingStartedDeb_t.html">link</a>, which describes the process for debian/ubuntu packages.</p>

<h1>Installation on Ubuntu 12.04:</h1>

<p>Lets make our host up to date:</p>

<pre><code>sudo apt-get update &amp;&amp; sudo apt-get upgrade -y
</code></pre>

<p>Before we will get Cassandra we need to install Java. I will go fo open-jdk:</p>

<pre><code>sudo apt-get install openjdk-7-jdk openjdk-7-jre -y
</code></pre>

<p>Lets check our java:</p>

<pre><code>java -version
</code></pre>

<p>You should get some thing like:</p>

<pre><code>java version "1.7.0_40"
Java(TM) SE Runtime Environment (build 1.7.0_40-b43)
Java HotSpot(TM) 64-Bit Server VM (build 24.0-b56, mixed mode)
</code></pre>

<p>Now we can get Cassandra:</p>

<pre><code>cd ~ &amp;&amp; wget http://apache.mirror1.spango.com/cassandra/2.0.2/apache-cassandra-2.0.2-bin.tar.gz
tar xvf apache-cassandra-2.0.2-bin.tar.gz
sudo mv apache-cassandra-2.0.2 /opt
</code></pre>

<p>Now C* looks like &ldquo;Installed&rdquo;, we still need to create startup script and configure some directories with correct permissions.</p>

<p>Since my interest is purely academic &ndash; I&rsquo;m not going to build HA solutions so far, but just want to play with C* a bit I will handle start/stop manually.</p>

<p>So, start daemon with:</p>

<pre><code>sudo /opt/apache-cassandra-2.0.2/bin/cassandra -f
</code></pre>

<p>After first run Cassandra will be happy to tell you that it created some dirs and files there and here. And it will start to listen some port, I belive that&rsquo;s 9160. Also -f option says foreground to daemon so it will attach to your screen and you probably need to open new one. BTW this one can be stopped with CTRL+C. Lets connect and check if everything works:</p>

<pre><code>/opt/apache-cassandra-2.0.2/bin/cqlsh
</code></pre>

<p>Should promt you next:</p>

<pre><code>Connected to Test Cluster at localhost:9160.
[cqlsh 4.1.0 | Cassandra 2.0.2 | CQL spec 3.1.1 | Thrift protocol 19.38.0]
Use HELP for help.
cqlsh&gt;
</code></pre>

<h1>Basics</h1>

<p>Cassandra operates with keyspaces &ndash; namespaces for tables. Which I think are databases in other DBMSes. Hehe, Cassandra is KSMS &ndash; keyspace management system.</p>

<p>Lets create one keyspace:</p>

<pre><code>CREATE KEYSPACE mykeyspace WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };  
</code></pre>

<p>So far that&rsquo;s black magick and we don&rsquo;t know anything about meaning of the words in command above.</p>

<p>To switch to newerly created keyspace do:</p>

<pre><code>USE mykeyspace;
</code></pre>

<p>Now we can create our first table. Example says it should be users. We don&rsquo;t have any problems with that, so:</p>

<pre><code>CREATE TABLE users (
  user_id int PRIMARY KEY,
  fname text,
  lname text
);
</code></pre>

<p>Unlike mongodb you have to explicitly specify the schema. As well as you have to create tables.</p>

<p>Lets insert some users. Again from examples:</p>

<pre><code>INSERT INTO users (user_id,  fname, lname)
  VALUES (1745, 'john', 'smith');
INSERT INTO users (user_id,  fname, lname)
  VALUES (1744, 'john', 'doe');
INSERT INTO users (user_id,  fname, lname)
  VALUES (1746, 'john', 'smith');
</code></pre>

<p>And get all the users:</p>

<pre><code>SELECT * FROM users;
</code></pre>

<p>If we want to search by particular field not primary key we need to specify an index on that field. Basically, If we want to query we need to have covered indexes, as far as I understand.</p>

<p>so without extra indexes:</p>

<pre><code>SELECT * FROM users WHERE lname = 'smith';
</code></pre>

<p>Will produce an error: Bad Request: No indexed columns present in by-columns clause with Equal operator.</p>

<p>Probably thats good, since from 1st query we can see already if we need indexes or not.</p>

<p>Adding indexes is very simple, just:</p>

<pre><code>CREATE INDEX ON users (lname);
</code></pre>

<p>After that it works like it should.</p>

<h2>Conclusions</h2>

<p>It was very fast and brief of how you can start with Cassandra on Ubuntu. I really hope that I will have more time to dig deeper and get actually what is so big deal about C*.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bitcoin planet]]></title>
    <link href="http://cthulhu.github.com/cthulhu/blog/2013/05/03/bitcoin-planet/"/>
    <updated>2013-05-03T12:27:00+02:00</updated>
    <id>http://cthulhu.github.com/cthulhu/blog/2013/05/03/bitcoin-planet</id>
    <content type="html"><![CDATA[<p><img src="http://cthulhu.github.com/cthulhu/images/bitcoins.jpg" title="[width] [height]" ></p>

<p>Last months there was a lot of noise around bitcoin topic. Personally I have purely technical and philosophic interest about that topic. Bitcoin is a phenomen. To be honest a lot of people still don&rsquo;t really understand importance of it for a progress. I  was really surprized to read articles about it in wallstreet journal blogs or so. Here is a bunch of links that might be useful. Ofcourse that is not complete list, but might be good starting point for topic research.</p>

<h2>Blogs:</h2>

<ul>
<li><a href="http://www.thebitcointrader.com/">http://www.thebitcointrader.com/</a> &ndash; bitcoin market</li>
<li><a href="http://www.bitcoinblogger.com/">http://www.bitcoinblogger.com/</a> &ndash; articles, might be that new content will not be published there any more</li>
<li><a href="http://blog.oleganza.com/">http://blog.oleganza.com/</a> &ndash; Quite nice blog about many aspects around bitcoins</li>
<li><a href="http://codinginmysleep.com/category/bitcoin/">http://codinginmysleep.com/</a> &ndash; a lot of information, news, technics etc</li>
<li><a href="http://themonetaryfuture.blogspot.nl/">THE MONETARY FUTURE</a> &ndash; a lot of articles, news, updates</li>
<li><a href="http://blog.cryptographyengineering.com/">A Few Thoughts on Cryptographic Engineering</a> &ndash; cryptograpy</li>
<li><a href="https://bitcoinfoundation.org/blog/">https://bitcoinfoundation.org/blog</a> &ndash; official blog of BT foundation</li>
<li><a href="http://techcrunch.com/tag/bitcoin/">BT on techcrunch</a></li>
<li><a href="http://www.reddit.com/r/Bitcoin/">BT on reddit</a></li>
</ul>


<h2>Twitter accounts to follow:</h2>

<ul>
<li><a href="https://twitter.com/BitcoinOz">@BitcoinOz</a></li>
<li><a href="https://twitter.com/BTCClassifieds">@BTCClassifieds</a></li>
<li><a href="https://twitter.com/BitHedger">@BitHedger</a></li>
<li><a href="https://twitter.com/bitcoininfo">@bitcoininfo</a></li>
<li><a href="https://twitter.com/BitcoinWatch">@BitcoinWatch</a></li>
<li><a href="https://twitter.com/matthew_d_green">@matthew_d_green</a></li>
</ul>


<p>And more. But a lot of them are just doing RT for news.</p>

<h2>Just links:</h2>

<ul>
<li><a href="http://www.chaum.com/articles/list_of_articles.htm">David Chaum&rsquo;s articles</a> &ndash; digital currencies, privacy, security, cryptology and more. Not much links but that can be starting poing for googling.</li>
</ul>


<p>I will add more links and twitter accounts from time to time, so stay tuned.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deployment of Ruby On Rails]]></title>
    <link href="http://cthulhu.github.com/cthulhu/blog/2013/04/19/deployment-of-ruby-on-rails/"/>
    <updated>2013-04-19T01:17:00+02:00</updated>
    <id>http://cthulhu.github.com/cthulhu/blog/2013/04/19/deployment-of-ruby-on-rails</id>
    <content type="html"><![CDATA[<p>From time to time I need to refresh a guide of Rails application production environment deployment.</p>

<p>My favourite OS is Linux Ubuntu. I prefer to use LTS versions, and today&rsquo;s LTS is 12.04 Precise Pangolin. You can download server distribution from <a href="http://ubuntu.com">Ubuntu official site</a>.</p>

<p>I will not cover OS installation process, since that&rsquo;s quite trivial and there are a lot of tutorials in the Internet about that. Besides, I can imagine that OS already installed on your VPS/PS/ whatever you have.</p>

<p><em>Note:</em> You might want to use chef or puppets for deployment automatization. And you are absolutelly right &ndash; recipes are the best way to DRY that as well. I will definitely go for it at some point.</p>

<p>So you have installed OS and logged in to the server via ssh.</p>

<p><em>Note:</em> Don&rsquo;t forget to disable passworded authorization and root login for your ssh server. Also it&rsquo;s nice to change ssh port from default one to something else, so the hackers will not be able to use simples atacks.</p>

<pre><code>sudo sed -i "/^Port / c Port 22222" /etc/ssh/sshd_config
sudo sed -i "/^PermitRootLogin / c PermitRootLogin no" /etc/ssh/sshd_config
</code></pre>

<p>Here is a set of commands you need to execute to install/configure needed software.</p>

<p>Let&rsquo;s make our system up to date:</p>

<pre><code>sudo apt-get update &amp;&amp; sudo apt-get upgrade
</code></pre>

<p>Now let&rsquo;s install software we will use (Git, compilers, archivers etc):</p>

<pre><code>sudo apt-get install git-core build-essential zlib1g-dev libssl-dev vim mc htop libmysqlclient-dev curl git-core ruby libpcre3-dev libxml2-dev libxml2 libxslt1-dev imagemagick libmagickcore-dev libmagickwand-dev autoconf byacc mysql-client libmysql-ruby libmysqlclient15-dev libmysqlclient-dev libmagickwand-dev imagemagick libmagickcore-dev zip screen curl nodejs install -y ghostscript imagemagick ffmpeg -y 
</code></pre>

<p>Now we need a web server. Personally I perfer nginx. It is fast, easy to use and configure. Someone prefers to installing from packages but I always do that from sources, since I can get the latest one and also I can configure modules I need.</p>

<p>So let&rsquo;s get it (at the moment of that blog the latest stable release is 1.2.8):</p>

<pre><code>sudo useradd -s /usr/sbin/nologin nginx
cd ~
wget http://nginx.org/download/nginx-1.2.8.tar.gz
tar -xvf nginx-1.2.8.tar.gz
cd nginx-1.2.8
./configure --prefix='/opt/nginx' --with-http_ssl_module --with-http_gzip_static_module --with-cc-opt='-Wno-error' --with-http_stub_status_module
make
sudo make install
cd ..
rm nginx-1.2.8 -rf
rm nginx-1.2.8.tar.gz
</code></pre>

<p>Few words about build configuration:</p>

<pre><code>--with-http_ssl_module - Application hosted there will have ssl encryption

--with-http_stub_status_module - I will use munin nginx-* plugins for requests monitoring etc.
</code></pre>

<p>You can read more about nginx configuration on <a href="http://nginx.org/">official site</a></p>

<p>Now we need a startup script (I have a gist for that):</p>

<div><script src='https://gist.github.com/5420306.js'></script>
<noscript><pre><code></code></pre></noscript></div>


<pre><code>cd ~
git clone https://gist.github.com/5420306.git
sudo cp 5420306/nginx /etc/init.d/nginx
sudo chmod 0755 /etc/init.d/nginx
rm 5420306 -rf
</code></pre>

<p>Now let&rsquo;s get ruby. I would recomend to use rvm even in production environment, since it will bring some flexibility to your setup. Also I recomend to use per user rvm installation rather than system wide.</p>

<pre><code> \curl -#L https://get.rvm.io | bash -s stable
</code></pre>

<p>Let&rsquo;s apply changes:</p>

<pre><code>source ~/.bashrc
</code></pre>

<p>And install latest (production) ruby version:</p>

<pre><code>rvm install 1.9.3-p392
</code></pre>

<p>I prefer to install release number rather than head. Since it&rsquo;s quite clear how to upgrade in case of new versions.</p>

<p>Now gemset (I prefer to use before setup hook for capistrano) but this also will work:</p>

<pre><code>rvm use 1.9.3@appname --create
</code></pre>

<p>Now let&rsquo;s connect nginx to our rails application. We can do that using several approaches (web servers). By popularity ATM:</p>

<ul>
<li><a href="http://code.macournoyer.com/thin/">Thin</a></li>
<li><a href="https://www.phusionpassenger.com/">Passenger</a></li>
<li><a href="http://unicorn.bogomips.org/">Unicorn</a></li>
<li><a href="http://pow.cx/">Pow</a></li>
<li><a href="http://puma.io/">Puma</a></li>
<li><a href="http://rainbows.rubyforge.org/">Rainbows</a> &ndash; Unicorn fork</li>
<li>Other options</li>
</ul>


<p>I perfer unicorn, but you might want to consider another option. Actualy I&rsquo;m quite happy to have so much choices. Not every platform have such a wide range of different solutions.</p>

<p>Unicorn is able to listen UNIX sockets, that&rsquo;s very handy and we can start multiple workers for our application and connect to nginx&rsquo;s upstream.</p>

<p>Our application will be located at /home/#{username}/application/#{application_name}, so socket location will be /home/#{username}/application/#{application_name}/shared/sockets/uncorn.sock.</p>

<p>Don&rsquo;t forget to add nginx user to your #{username} group. For example, for user &lsquo;deployer&rsquo; it will be:</p>

<pre><code>sudo usermod -a -G deployer nginx
</code></pre>

<p>Last stage is database. I use mysql in most cases. Here how you can get it:</p>

<pre><code>sudo apt-get install mysql-server mysql-client mysql-common
</code></pre>

<p>I will not cover mysql configuration here, since It is really depends on your cases.
Let&rsquo;s created database and user:</p>

<pre><code>mysql -u root -p -e "CREATE DATABASE \`application_production\` CHARACTER SET utf8 COLLATE utf8_general_ci;"
mysql -u root -p -e "CREATE USER 'application'@'localhost' IDENTIFIED BY 'some_pass';"
mysql -u root -p -e "GRANT ALL PRIVILEGES ON application_production.* TO 'application'@'localhost' WITH GRANT OPTION;""
</code></pre>

<p>That&rsquo;s it. Since now I have step by step tutorial, I will try to compose a chef recipes for that.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elastic MapReduce using streaming and ruby]]></title>
    <link href="http://cthulhu.github.com/cthulhu/blog/2013/03/04/elastic-mapreduce-using-streaming-and-ruby/"/>
    <updated>2013-03-04T01:57:00+01:00</updated>
    <id>http://cthulhu.github.com/cthulhu/blog/2013/03/04/elastic-mapreduce-using-streaming-and-ruby</id>
    <content type="html"><![CDATA[<p><a href="http://aws.amazon.com/elasticmapreduce/">Amazon Elastic MapReduce</a> &ndash; Is a service provided by amazon web services intended to provide facilities to build fast and easy to maintain big data calculations using MapReduce paradigm. Basically, Amazon EMR is a hosted Hadoop, at first glance. But in combination with aws API and other services, like S3, EMR becomes quite feature full solution. Hadoop requires some extra skills to deploy it locally. At least you need to be familar with JVM infrostructure. EMR allows you to abstract from some technical aspects like deploying cluster and managing nodes.</p>

<p>Also, which is nice about EMR that you can write MR functions in any language, not necessary in Java.</p>

<p>Before we will start to work on the topic itself, here are some requirements you need to meet to be able to use samples:</p>

<ul>
<li>  Ruby programming language skills &ndash; not much, since ruby is quite expressive language</li>
<li>  Amazon Web Services account &ndash; if you don&rsquo;t have one please create, subscribe for EMR (Elastic MapReduce) and S3</li>
</ul>


<p>The main goal is to be able to process huge amount of data (we will take log files from nginx server from one of my applications) and generate statistic.
We will investigate amount of hits per browser families per day. I would like to note that it is just an example of how you can use amazon EMR for such kind of cases.</p>

<p>About MapReduce concept you can read on different sources, starting from <a href="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/uk//archive/mapreduce-osdi04.pdf">Goolge&rsquo;s whitepaper</a>, <a href="http://en.wikipedia.org/wiki/MapReduce">wiki</a>, <a href="http://hadoop.apache.org/docs/r1.0.4/mapred_tutorial.html">Hadoop sources</a>.</p>

<p>I will not cover the theoretical principles of MR, since they covered quite well in sources above. Besides, I think It is easier to get that on realworld example. So please, read the references, at least article in wikipedia.</p>

<p>First our source file &ndash; I will use nginx access.log from demo server and the use case will be is to collect amount of hits per browser family. Basilcaly, I dont care about period, let&rsquo;s say we have 1 log file per day and we will use as input file that one only.</p>

<p>Every access.log line formated with &ldquo;combined&rdquo; format:</p>

<pre><code>'$remote_addr - $remote_user [$time_local]  '
    '"$request" $status $body_bytes_sent '
    '"$http_referer" "$http_user_agent"';
</code></pre>

<p>And in real life looks like this:</p>

<pre><code>127.0.0.1 - - [23/Apr/2013:00:10:02 +0100] "GET / HTTP/1.0" 403 168 "-" "Wget/1.12 (linux-gnu)"
193.169.146.189 - - [22/Apr/2013:09:07:12 +0100] "GET /login HTTP/1.1" 200 1651 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_3) AppleWebKit/536.28.10 (KHTML, like Gecko) Version/6.0.3 Safari/536.28.10"
83.161.144.255 - - [22/Apr/2013:09:08:22 +0100] "GET /login HTTP/1.1" 200 1648 "-" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31"
</code></pre>

<p>Your log files should be placed on amazon S3 so EMR will be able to get access to them.</p>

<p>For my test cases I used 1 bucket with next structure inside:</p>

<pre><code>bootstrap.sh  
browsers.rb
input  
output
</code></pre>

<p>bootstrap.sh &ndash; bash file for software installation on every EMR node. Basically, you can setup gems and libs, compile needed externall resources etc with it.</p>

<p>My file is very simple:</p>

<pre><code>#!/bin/bash

sudo apt-get -y install rubygems
</code></pre>

<p>Since rubygems depends on ruby, we will get it installed as well. I can imagine that in reallife cases bootstrap scripts will be much more complex.</p>

<p>browsers.rb &ndash; that is basically our map function. In my example it looks like:</p>

<div><script src='https://gist.github.com/5446482.js'></script>
<noscript><pre><code></code></pre></noscript></div>


<p>That example will never win first price, but any way it works and shows main principles.</p>

<p>The main idea behind is that data actually goes to mapper STDIN line by line, after that we parse the lines and generate the emission to the STDOUT. It is quit simple and can be done actually in any language even shell.</p>

<p>To get started go to aws console, your favourite region and select s3. After that create bucket, in my case it was emr.workshop. Upload workshop directory with content above to that bucket. As a result you should see some thing like this:</p>

<p><img src="http://cthulhu.github.com/cthulhu/images/s3-emr.jpg" title="[width] [height]" ></p>

<p>Input folder should contain access.log file with data.</p>

<p><em>Note:</em> I prefer to create a separate user and group when do some actions in aws. That guaranties some rights and visibility isolation, especially when you have a lot of critical and important applications running under same account. You can easily do that with IAM (Identity and Access Management) and policies generators. I will not cover that topic here, since it&rsquo;s nice described in aws documentation.</p>

<p>Once we loaded input and created policies (you can skip that if you want), we can build our MR-job.</p>

<p>Go to aws elastic map reduce and click &ldquo;Create New Job Flow&rdquo;. Fill the name of the job, in my case is &ldquo;Browsers Analysis&rdquo;, select hadoop version &ldquo;Amazon Distribution&rdquo;, select &ldquo;Run your own application&rdquo; as a job flow.</p>

<p>Result should be like that:</p>

<p><img src="http://cthulhu.github.com/cthulhu/images/create-job-step1.jpg" title="[width] [height]" ></p>

<p>On the next step you should fill in MR-job parameters. For me it was:</p>

<ul>
<li>Input location: s3://emr-workshop2013/input/access.log</li>
<li>Output location: s3://emr-workshop2013/output/</li>
<li>Mapper: s3://emr-workshop2013/browsers.rb</li>
<li>Reducer: aggregate</li>
</ul>


<p><img src="http://cthulhu.github.com/cthulhu/images/create-job-step2.jpg" title="[width] [height]" ></p>

<p>Next you need to pick up hardware for the job. That is really depends on your case and expectations. If you have 100 Gb log file, and you want to process it faster than you probably need to think about more nodes. In our example everything is quite simple and log file is not very big so we will pick up few small instances(basically default options). You should also consider <a href="http://aws.amazon.com/elasticmapreduce/pricing/">costs</a> when you pick up size of computation cluster.</p>

<p>On Advanced options step you need to configure additional options for the job. I would say use defaults and press continue, since in test example all those settings do not matter much.</p>

<p>On next step you will have to configure bootstrap options for your nodes. All our nodes will be bootstrapped same using our script loaded to s3 bucket above. So Select &ldquo;Configure your Bootstrap Actions&rdquo;, Action type: &ldquo;Custom Action&rdquo;, Name: Node bootstrap. Amazon S3 Location: s3://emr-workshop2013/bootstrap.sh.</p>

<p><img src="http://cthulhu.github.com/cthulhu/images/create-job-step5.jpg" title="[width] [height]" ></p>

<p>On last step of the wisard you will be able to review job&rsquo;s settings.</p>

<p>After you will press &ldquo;Create Job Flow&rdquo; Amazon will start instances and the computation will start. It will take few minutes to bootstrap the nodes, and ofcourse some time to process the data from input log file. As a result in output folder you will have a bunch of text files with aggregated counts per browser family.</p>

<p>How to debug MR-job?</p>

<p>Since job(mapper) gets data from STDIN you can easily do that with just piping in linux command line:</p>

<pre><code>cat input/access.log | ./browsers.rb
</code></pre>

<p>will give you output from the mapper, and with:</p>

<pre><code>cat input/access.log | ./browsers.rb | sort | uniq -c
</code></pre>

<p>You will have results, close to aggregate reducer output. In my case it was:</p>

<pre><code>      1 LongValueSum:Chrome ["10"]  1
     14 LongValueSum:Chrome ["11"]  1
      2 LongValueSum:Chrome ["12"]  1
      1 LongValueSum:Chrome ["14"]  1
      1 LongValueSum:Chrome ["15"]  1
      2 LongValueSum:Chrome ["16"]  1
      3 LongValueSum:Chrome ["17"]  1
     90 LongValueSum:Chrome ["18"]  1
      1 LongValueSum:Chrome ["20"]  1
      7 LongValueSum:Chrome ["21"]  1
     25 LongValueSum:Chrome ["22"]  1
   1826 LongValueSum:Chrome ["23"]  1
     10 LongValueSum:Chrome ["24"]  1
      3 LongValueSum:Chrome ["25"]  1
      1 LongValueSum:Chrome ["5"]   1
      1 LongValueSum:Chrome ["7"]   1
      1 LongValueSum:Chromium ["20"]    1
      1 LongValueSum:Chromium ["22"]    1
     15 LongValueSum:Firefox ["10"] 1
      8 LongValueSum:Firefox ["11"] 1
     26 LongValueSum:Firefox ["12"] 1
     13 LongValueSum:Firefox ["13"] 1
     30 LongValueSum:Firefox ["14"] 1
     23 LongValueSum:Firefox ["15"] 1
     56 LongValueSum:Firefox ["16"] 1
    940 LongValueSum:Firefox ["17"] 1
     31 LongValueSum:Firefox ["18"] 1
      2 LongValueSum:Firefox ["19"] 1
      1 LongValueSum:Firefox ["20"] 1
     22 LongValueSum:Firefox ["3"]  1
      1 LongValueSum:Firefox ["4"]  1
      1 LongValueSum:Firefox ["5"]  1
      4 LongValueSum:Firefox ["6"]  1
     27 LongValueSum:Firefox ["8"]  1
      9 LongValueSum:Firefox ["9"]  1
   3042 LongValueSum:Internet Explorer  1
     96 LongValueSum:Opera ["9"]    1
     65 LongValueSum:Safari     1
      1 LongValueSum:Safari ["3"]   1
    953 LongValueSum:Safari ["4"]   1
    683 LongValueSum:Safari ["5"]   1
   1407 LongValueSum:Safari ["6"]   1
     12 LongValueSum:Safari ["7"]   1
</code></pre>

<p>Also during MR-job creation you will be able to set bucket for MR process logging and ability to access nodes to check software etc(Step Advanced Settings).</p>

<p>A few words about <a href="http://aws.amazon.com/elasticmapreduce/pricing/">pricing</a>. First of all you should consider that EMR will normalize hours of usage, so if you job run for 20 minutes you will pay for a full hour per every node. So calculations are next: Total amount of nodes of type I * full hours of job running time * ( hourly rate for type I EC2 instance +  hourly rate for MR job on type I instance) . So our test example wich was running on 3 small nodes for 15 minutes in EU-West region costs = 3 * 1 * ( 0.065 + 0.015 ) + S3 costs = $0.24 + S3 costs. It is hard to calculate S3 costs for that particluar job since it was small amount of data and bandwidth was used.</p>

<p>This was very basic overview of EMR abilities, I hope in the nearest future I will be able to present some more complex usecases and applications of that technologie.</p>
]]></content>
  </entry>
  
</feed>
